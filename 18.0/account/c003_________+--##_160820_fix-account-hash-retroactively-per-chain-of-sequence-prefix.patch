PR: https://github.com/odoo/odoo/pull/160820

From: b74c7a4fd20fd30d420ba55225a546b0dc61835e
From: Ricardo Gomes Rodrigues
Date: 2024-04-06 18:01:50

Structural Changes: 5
Total Changes: 452

[FIX] account: hash retroactively per chain of sequence_prefix

Since 8cc20fb, we hash the invoices on Send&Print instead of when posting.

This may result in weird behaviour for instance when we:
- Post INV/1
- Post INV/2
- Post + Send INV/3
- Send INV/2

The hash of INV/2 will depend on INV/3, which is not what we want. This
will also look weird in the inalterability report where the first move
hashed might be the last move of the sequence and vice-versa.

To fix this, we will now retroactively hash the moves on Send&Print for
invoices, and on post for bills. In the previous example, this means
that when we hash INV/3, we should first hash INV/1 then INV/2 and then
INV/3.

For invoices, if the user does not want to Send&Print the invoice but
wants to have a hash, he/she can click on the button to hash the invoice
which will also retroactively hash the previous moves.

This retroactive hashing is not sufficient for cases where we have a mix
of invoices with different sequence_prefix.

From now on, we will have a hash chain per sequence_prefix
instead of per journal. If, for instance, one uses a series such as
INV/2024/00001, INV/2024/00002, ..., the sequence_prefix is INV/2024,
and therefore we will have one hash chain per year.

This also allows us to get rid of the secure_sequence_id, and the
secure_sequence_number, and we can only depend on the sequence_number.

However, we have to keep the secure_sequence_number field for the
integrity verification of moves previous to this commit because it
defines the order on which the moves were hashed.

We also now show a warning on the Accounting dashboard if the journal has
unhashed entries that redirects the user to the moves that need to be hashed.

task-id 3820975

closes odoo/odoo#160820

Related: odoo/enterprise#60221
Signed-off-by: William Andr√© (wan) <wan@odoo.com>
Signed-off-by: Ricardo Gomes Rodrigues (rigr) <rigr@odoo.com>

================================= pseudo patch: =================================

--- a/addons/account/models/account_journal.py
+++ b/addons/account/models/account_journal.py
@@ -102,8 +102,8 @@ class AccountJournal(models.Model):
              "allowing finding the right account.", string='Suspense Account',
         domain="[('deprecated', '=', False), ('account_type', '=', 'asset_current')]",
     )
-    restrict_mode_hash_table = fields.Boolean(string="Lock Sent Entries with Hash",
-        help="If ticked, the accounting entry or invoice receives a hash as soon as it is sent and cannot be modified anymore.")
+    restrict_mode_hash_table = fields.Boolean(string="Lock Sent Invoices with Hash",
+        help="If ticked, when the invoice is sent, the hash chain will be computed from the last move hashed to the new move to be hashed. The hash can also be performed on demand.")
     sequence = fields.Integer(help='Used to order Journals in the dashboard view', default=10)
 
     invoice_reference_type = fields.Selection(string='Communication Type', required=True, selection=[('none', 'Open'), ('partner', 'Based on Customer'), ('invoice', 'Based on Invoice')], default='invoice', help='You can set here the default communication that will appear on customer invoices, once validated, to help the customer to refer to that particular invoice when making the payment.')
@@ -539,7 +539,7 @@ class AccountJournal(models.Model):
                         raise UserError(_("The partners of the journal's company and the related bank account mismatch."))
             if 'restrict_mode_hash_table' in vals and not vals.get('restrict_mode_hash_table'):
                 domain = self.env['account.move']._get_move_hash_domain(
-                    common_domain=[('journal_id', '=', self.id), ('secure_sequence_number', '!=', 0)]
+                    common_domain=[('journal_id', '=', self.id), ('inalterable_hash', '!=', False)]
                 )
                 journal_entry = self.env['account.move'].sudo().search_count(domain, limit=1)
                 if journal_entry:
@@ -566,9 +566,6 @@ class AccountJournal(models.Model):
         if 'bank_acc_number' in vals:
             for journal in self.filtered(lambda r: r.type == 'bank' and not r.bank_account_id):
                 journal.set_bank_account(vals.get('bank_acc_number'), vals.get('bank_id'))
-        for record in self:
-            if record.restrict_mode_hash_table and not record.secure_sequence_id:
-                record._create_secure_sequence(['secure_sequence_id'])
 
         return result
 
@@ -712,10 +709,6 @@ class AccountJournal(models.Model):
             if journal.type == 'bank' and not journal.bank_account_id and vals.get('bank_acc_number'):
                 journal.set_bank_account(vals.get('bank_acc_number'), vals.get('bank_id'))
 
-            # Create the secure_sequence_id if necessary
-            if journal.restrict_mode_hash_table and not journal.secure_sequence_id:
-                journal._create_secure_sequence(['secure_sequence_id'])
-
         return journals
 
     def set_bank_account(self, acc_number, bank_id=None):
@@ -827,28 +820,6 @@ class AccountJournal(models.Model):
             })
         return action_vals
 
-    def _create_secure_sequence(self, sequence_fields):
-        """This function creates a no_gap sequence on each journal in self that will ensure
-        a unique number is given to all posted account.move in such a way that we can always
-        find the previous move of a journal entry on a specific journal.
-        """
-        for journal in self:
-            vals_write = {}
-            for seq_field in sequence_fields:
-                if not journal[seq_field]:
-                    vals = {
-                        'name': _('Securisation of %s - %s', seq_field, journal.name),
-                        'code': 'SECUR%s-%s' % (journal.id, seq_field),
-                        'implementation': 'no_gap',
-                        'prefix': '',
-                        'suffix': '',
-                        'padding': 0,
-                        'company_id': journal.company_id.id}
-                    seq = self.env['ir.sequence'].create(vals)
-                    vals_write[seq_field] = seq.id
-            if vals_write:
-                journal.write(vals_write)
-
     # -------------------------------------------------------------------------
     # REPORTING METHODS
     # -------------------------------------------------------------------------

--- a/addons/account/models/account_journal_dashboard.py
+++ b/addons/account/models/account_journal_dashboard.py
@@ -32,6 +32,7 @@ class account_journal(models.Model):
     has_statement_lines = fields.Boolean(compute='_compute_current_statement_balance') # technical field used to avoid computing the value multiple times
     entries_count = fields.Integer(compute='_compute_entries_count')
     has_sequence_holes = fields.Boolean(compute='_compute_has_sequence_holes')
+    has_unhashed_entries = fields.Boolean(string='Unhashed Entries', compute='_compute_has_unhashed_entries')
     last_statement_id = fields.Many2one(comodel_name='account.bank.statement', compute='_compute_last_bank_statement')
 
     def _compute_current_statement_balance(self):
@@ -144,11 +145,32 @@ class account_journal(models.Model):
         })
         return self.env.cr.fetchall()
 
+    def _get_moves_to_hash(self, include_pre_last_hash, early_stop):
+        """
+        If we have INV/1, INV/2 not hashed, then INV/3, INV/4 hashed, then INV/5 and INV/6 not hashed
+        :param include_pre_last_hash: if True, this will include INV/1 and INV/2. Otherwise not.
+        :param early_stop: if True, stop searching when we found at least one record
+        :return:
+        """
+        return self.env['account.move'].search([
+            ('restrict_mode_hash_table', '=', True),
+            ('inalterable_hash', '=', False),
+            ('journal_id', '=', self.id),
+            ('date', '>', self.company_id._get_user_fiscal_lock_date()),
+        ])._get_chains_to_hash(force_hash=True, raise_if_gap=False, raise_if_no_document=False, early_stop=early_stop, include_pre_last_hash=include_pre_last_hash)
+
     def _compute_has_sequence_holes(self):
         has_sequence_holes = set(journal_id for journal_id, _prefix in self._query_has_sequence_holes())
         for journal in self:
             journal.has_sequence_holes = journal.id in has_sequence_holes
 
+    def _compute_has_unhashed_entries(self):
+        for journal in self:
+            if journal.restrict_mode_hash_table:
+                journal.has_unhashed_entries = journal._get_moves_to_hash(include_pre_last_hash=False, early_stop=True)
+            else:
+                journal.has_unhashed_entries = False
+
     def _compute_entries_count(self):
         res = {
             journal.id: count
@@ -534,6 +556,7 @@ class account_journal(models.Model):
                 'sum_late': currency.format(sum_late),
                 'has_sequence_holes': journal.has_sequence_holes,
                 'title_has_sequence_holes': title_has_sequence_holes,
+                'has_unhashed_entries': journal.has_unhashed_entries,
                 'is_sample_data': dashboard_data[journal.id]['entries_count'],
             })
 
@@ -884,6 +907,24 @@ class account_journal(models.Model):
             }
         }
 
+    def show_unhashed_entries(self):
+        self.ensure_one()
+        chains_to_hash = self._get_moves_to_hash(include_pre_last_hash=True, early_stop=False)
+        moves = self.env['account.move'].concat(*[chain_moves['moves'] for chain_moves in chains_to_hash])
+        action = {
+            'type': 'ir.actions.act_window',
+            'name': _('Journal Entries to Hash'),
+            'res_model': 'account.move',
+            'domain': [('id', 'in', moves.ids)],
+            'views': [(False, 'tree'), (False, 'form')],
+        }
+        if len(moves.ids) == 1:
+            action.update({
+                'res_id': moves[0].id,
+                'views': [(False, 'form')],
+            })
+        return action
+
     def create_bank_statement(self):
         """return action to create a bank statements. This button should be called only on journals with type =='bank'"""
         action = self.env["ir.actions.actions"]._for_xml_id("account.action_bank_statement_tree")

--- a/addons/account/models/account_move.py
+++ b/addons/account/models/account_move.py
@@ -36,7 +36,7 @@ from odoo.tools import (
 _logger = logging.getLogger(__name__)
 
 
-MAX_HASH_VERSION = 3
+MAX_HASH_VERSION = 4
 
 PAYMENT_STATE_SELECTION = [
         ('not_paid', 'Not Paid'),
@@ -259,8 +259,8 @@ class AccountMove(models.Model):
 
     # === Hash Fields === #
     restrict_mode_hash_table = fields.Boolean(related='journal_id.restrict_mode_hash_table')
-    secure_sequence_number = fields.Integer(string="Inalteralbility No Gap Sequence #", readonly=True, copy=False, index=True)
-    inalterable_hash = fields.Char(string="Inalterability Hash", readonly=True, copy=False)
+    secure_sequence_number = fields.Integer(string="Inalterability No Gap Sequence #", readonly=True, copy=False, index=True)
+    inalterable_hash = fields.Char(string="Inalterability Hash", readonly=True, copy=False, index='btree_not_null')
     string_to_hash = fields.Char(compute='_compute_string_to_hash', readonly=True)
 
     # ==============================================================================================
@@ -2711,10 +2711,13 @@ class AccountMove(models.Model):
             return True
         self._sanitize_vals(vals)
         for move in self:
-            if (self._is_move_restricted(move) and set(vals).intersection(move._get_integrity_hash_fields())):
-                raise UserError(_("You cannot edit the following fields due to restrict mode being activated on the journal: %s.", ', '.join(move._get_integrity_hash_fields())))
-            if (move.restrict_mode_hash_table and move.inalterable_hash and 'inalterable_hash' in vals) or (move.secure_sequence_number and 'secure_sequence_number' in vals):
-                raise UserError(_('You cannot overwrite the values ensuring the inalterability of the accounting.'))
+            violated_fields = set(vals).intersection(move._get_integrity_hash_fields() + ['inalterable_hash'])
+            if move.inalterable_hash and violated_fields:
+                raise UserError(_(
+                    "This document is protected by a hash. "
+                    "Therefore, you cannot edit the following fields: %s.",
+                    ', '.join(f['string'] for f in self.fields_get(violated_fields).values())
+                ))
             if (move.posted_before and 'journal_id' in vals and move.journal_id.id != vals['journal_id']):
                 raise UserError(_('You cannot edit the journal of an account move if it has been posted once.'))
             if (move.name and move.name != '/' and move.sequence_number not in (0, 1) and 'journal_id' in vals and move.journal_id.id != vals['journal_id']):
@@ -2765,16 +2768,11 @@ class AccountMove(models.Model):
                     posted_move._check_fiscalyear_lock_date()
                     posted_move.line_ids._check_tax_lock_date()
 
-                # Hash the moves that fit the move hash domain
-                if vals.get('is_move_sent') or vals.get('state') == 'posted':
-                    self.flush_recordset()  # Ensure that the name is correctly computed before it is used to generate the hash
-                    filter_domain = self._get_move_hash_domain(common_domain=[('inalterable_hash', '=', False)])
-                    for move in self.filtered_domain(filter_domain).sorted(lambda m: (m.date, m.ref or '', m.id)):
-                        new_number = move.journal_id.secure_sequence_id.next_by_id()
-                        res |= super(AccountMove, move).write({
-                            'secure_sequence_number': new_number,
-                            'inalterable_hash': move._get_new_hash(new_number),
-                        })
+                if vals.get('state') == 'posted':
+                    self.flush_recordset()  # Ensure that the name is correctly computed
+
+                if vals.get('is_move_sent'):
+                    self._hash_moves()
 
             self._synchronize_business_models(set(vals.keys()))
 
@@ -3210,78 +3208,135 @@ class AccountMove(models.Model):
         hash_version = self._context.get('hash_version', MAX_HASH_VERSION)
         if hash_version == 1:
             return ['date', 'journal_id', 'company_id']
-        elif hash_version in (2, 3):
+        elif hash_version in (2, 3, 4):
             return ['name', 'date', 'journal_id', 'company_id']
         raise NotImplementedError(f"hash_version={hash_version} doesn't exist")
 
     def _get_integrity_hash_fields_and_subfields(self):
         return self._get_integrity_hash_fields() + [f'line_ids.{subfield}' for subfield in self.line_ids._get_integrity_hash_fields()]
 
-    def _get_new_hash(self, secure_seq_number):
-        """ Returns the hash to write on journal entries when they get posted"""
-        self.ensure_one()
-        #get the only one exact previous move in the securisation sequence
-        prev_move = self.sudo().search([('state', '=', 'posted'),
-                                 ('company_id', '=', self.company_id.id),
-                                 ('journal_id', '=', self.journal_id.id),
-                                 ('secure_sequence_number', '!=', 0),
-                                 ('secure_sequence_number', '=', int(secure_seq_number) - 1)])
-        if prev_move and len(prev_move) != 1:
-            raise UserError(
-               _('An error occurred when computing the inalterability. Impossible to get the unique previous posted journal entry.'))
-
-        #build and return the hash
-        return self._compute_hash(prev_move.inalterable_hash if prev_move else u'')
-
     @api.model
-    def _get_move_hash_domain(self, common_domain=False):
-        # hash customer invoices/refunds on send, the rest on post
-        out_types = self.get_sale_types(include_receipts=True)
-        common_domain = (common_domain or []) + [('restrict_mode_hash_table', '=', True)]
-        return expression.AND(
-            [common_domain, expression.OR([
-                [('move_type', 'in', out_types), ('is_move_sent', '=', True)],
-                [('move_type', 'not in', out_types), ('state', '=', 'posted')],
-            ])]
-        )
+    def _get_move_hash_domain(self, common_domain=False, force_hash=False):
+        common_domain = expression.AND([
+            common_domain or [],
+            [('restrict_mode_hash_table', '=', True)]
+        ])
+        if force_hash:
+            return expression.AND([common_domain, [('state', '=', 'posted')]])
+        return expression.AND([
+            common_domain,
+            [('move_type', 'in', self.get_sale_types(include_receipts=True)), ('is_move_sent', '=', True)]
+        ])
 
     @api.model
-    def _is_move_restricted(self, move):
-        return move.filtered_domain(self._get_move_hash_domain())
+    def _is_move_restricted(self, move, force_hash=False):
+        return move.filtered_domain(self._get_move_hash_domain(force_hash=force_hash))
+
+    def _hash_moves(self, force_hash=False):
+        chains_to_hash = self._get_chains_to_hash(force_hash=force_hash)
+        for chain in chains_to_hash:
+            move_hashes = chain['moves']._calculate_hashes(chain['previous_hash'])
+            for move, move_hash in move_hashes.items():
+                move.inalterable_hash = move_hash
+            chain['moves']._message_log_batch(bodies={m.id: _("This move has been locked.") for m in chain['moves']})
+
+    def _get_chains_to_hash(self, force_hash=False, raise_if_gap=True, raise_if_no_document=True, include_pre_last_hash=False, early_stop=False):
+        """
+        From a recordset of moves, retrieve the chains of moves that need to be hashed by taking
+        into account the last move of each chain of the recordset.
+        So if we have INV/1, INV/2, INV/3, INV4 that are not hashed yet in the database
+        but self contains INV/2, INV/3, we will return INV/1, INV/2 and INV/3. Not INV/4.
+        :param force_hash: if True, we'll check all moves posted, independently of whether they were sent or not
+        :param raise_if_gap: if True, we'll raise an error if a gap is detected in the sequence
+        :param raise_if_no_document: if True, we'll raise an error if no document needs to be hashed
+        :param include_pre_last_hash: if True, we'll include the moves not hashed that are previous to the last hashed move
+        :param early_stop: if True, we'll stop the computation as soon as we find at least one document to hash
+        """
+        res = []  # List of dict {'previous_hash': str, 'moves': recordset}
+        for journal, journal_moves in self.grouped('journal_id').items():
+            for chain_moves in journal_moves.grouped('sequence_prefix').values():
+                last_move_in_chain = chain_moves.sorted('sequence_number')[-1]
+                if not self._is_move_restricted(last_move_in_chain, force_hash=force_hash):
+                    continue
 
-    def _compute_hash(self, previous_hash):
-        """ Computes the hash of the browse_record given as self, based on the hash
-        of the previous record in the company's securisation sequence given as parameter"""
-        self.ensure_one()
-        hash_string = sha256((previous_hash + self.string_to_hash).encode('utf-8'))
-        return hash_string.hexdigest()
-
-    @api.depends(lambda self: self._get_integrity_hash_fields_and_subfields())
-    @api.depends_context('hash_version')
-    def _compute_string_to_hash(self):
-        def _getattrstring(obj, field_str):
-            hash_version = self._context.get('hash_version', MAX_HASH_VERSION)
-            field_value = obj[field_str]
-            if obj._fields[field_str].type == 'many2one':
+                common_domain = [
+                    ('journal_id', '=', journal.id),
+                    ('sequence_prefix', '=', last_move_in_chain.sequence_prefix),
+                ]
+                last_move_hashed = self.env['account.move'].search([
+                    *common_domain,
+                    ('inalterable_hash', '!=', False),
+                ], order='sequence_number desc', limit=1)
+
+                domain = self.env['account.move']._get_move_hash_domain([
+                    *common_domain,
+                    ('sequence_number', '<=', last_move_in_chain.sequence_number),
+                    ('inalterable_hash', '=', False),
+                    ('date', '>', last_move_in_chain.company_id._get_user_fiscal_lock_date()),
+                ], force_hash=True)
+                if last_move_hashed and not include_pre_last_hash:
+                    # Hash moves only after the last hashed move, not the ones that may have been posted before the journal was set on restrict mode
+                    domain.extend([('sequence_number', '>', last_move_hashed.sequence_number)])
+
+                # On the accounting dashboard, we are only interested on whether there are documents to hash or not
+                # so we can stop the computation early if we find at least one document to hash
+                if early_stop:
+                    if self.env['account.move'].sudo().search_count(domain, limit=1):
+                        return True
+                    continue
+                moves_to_hash = self.env['account.move'].sudo().search(domain, order='sequence_number')
+                if not moves_to_hash and force_hash and raise_if_no_document:
+                    raise UserError(_(
+                        "This move could not be locked either because:\n"
+                        "- some move with the same sequence prefix has a higher number. You may need to resequence it.\n"
+                        "- the move's date is anterior to the lock date"
+                    ))
+                if raise_if_gap and moves_to_hash and moves_to_hash[0].sequence_number + len(moves_to_hash) - 1 != moves_to_hash[-1].sequence_number:
+                    raise UserError(_(
+                        "An error occurred when computing the inalterability. A gap has been detected in the sequence."
+                    ))
+
+                res.append({
+                    'previous_hash': last_move_hashed.inalterable_hash,
+                    'moves': moves_to_hash.sudo(False),
+                })
+        if early_stop:
+            return False
+        return res
+
+    def _calculate_hashes(self, previous_hash):
+        """
+        :return: dict of move_id: hash
+        """
+        hash_version = self._context.get('hash_version', MAX_HASH_VERSION)
+
+        def _getattrstring(obj, field_name):
+            field_value = obj[field_name]
+            if obj._fields[field_name].type == 'many2one':
                 field_value = field_value.id
-            if obj._fields[field_str].type == 'monetary' and hash_version >= 3:
+            if obj._fields[field_name].type == 'monetary' and hash_version >= 3:
                 return float_repr(field_value, obj.currency_id.decimal_places)
             return str(field_value)
 
+        move2hash = {}
+        previous_hash = previous_hash or ''
+
         for move in self:
+            if previous_hash and previous_hash.startswith("$"):
+                previous_hash = previous_hash.split("$")[2]  # The hash version is not used for the computation of the next hash
             values = {}
-            for field in move._get_integrity_hash_fields():
-                values[field] = _getattrstring(move, field)
+            for fname in move._get_integrity_hash_fields():
+                values[fname] = _getattrstring(move, fname)
 
             for line in move.line_ids:
-                for field in line._get_integrity_hash_fields():
-                    k = 'line_%d_%s' % (line.id, field)
-                    values[k] = _getattrstring(line, field)
-            #make the json serialization canonical
-            #  (https://tools.ietf.org/html/draft-staykov-hu-json-canonical-form-00)
-            move.string_to_hash = dumps(values, sort_keys=True,
-                                                ensure_ascii=True, indent=None,
-                                                separators=(',', ':'))
+                for fname in line._get_integrity_hash_fields():
+                    k = 'line_%d_%s' % (line.id, fname)
+                    values[k] = _getattrstring(line, fname)
+            current_record = dumps(values, sort_keys=True, ensure_ascii=True, indent=None, separators=(',', ':'))
+            hash_string = sha256((previous_hash + current_record).encode('utf-8')).hexdigest()
+            move2hash[move] = f"${hash_version}${hash_string}" if hash_version >= 4 else hash_string
+            previous_hash = move2hash[move]
+        return move2hash
 
     # -------------------------------------------------------------------------
     # RECURRING ENTRIES
@@ -4426,7 +4481,7 @@ class AccountMove(models.Model):
                 # so we also check tax_cash_basis_origin_move_id, which stays unchanged
                 # (we need both, as tax_cash_basis_origin_move_id did not exist in older versions).
                 raise UserError(_('You cannot reset to draft a tax cash basis journal entry.'))
-            if self._is_move_restricted(move):
+            if move.inalterable_hash:
                 raise UserError(_('You cannot modify a sent entry of this journal because it is in strict mode.'))
             # We remove all the analytics entries for this journal
             move.mapped('line_ids.analytic_line_ids').unlink()
@@ -4434,6 +4489,9 @@ class AccountMove(models.Model):
         self.mapped('line_ids').remove_move_reconcile()
         self.write({'state': 'draft', 'is_move_sent': False})
 
+    def button_hash(self):
+        self._hash_moves(force_hash=True)
+
     def button_request_cancel(self):
         """ Hook allowing the localizations to request a cancellation from the government before cancelling the invoice. """
         self.ensure_one()

--- a/addons/account/models/account_move_line.py
+++ b/addons/account/models/account_move_line.py
@@ -1559,7 +1559,7 @@ class AccountMoveLine(models.Model):
         if account_to_write and account_to_write.deprecated:
             raise UserError(_('You cannot use a deprecated account.'))
 
-        inalterable_fields = set(self._get_integrity_hash_fields()).union({'inalterable_hash', 'secure_sequence_number'})
+        inalterable_fields = set(self._get_integrity_hash_fields()).union({'inalterable_hash'})
         hashed_moves = self.move_id.filtered('inalterable_hash')
         violated_fields = set(vals) & inalterable_fields
         if hashed_moves and violated_fields:
@@ -3051,7 +3051,7 @@ class AccountMoveLine(models.Model):
         hash_version = self._context.get('hash_version', MAX_HASH_VERSION)
         if hash_version == 1:
             return ['debit', 'credit', 'account_id', 'partner_id']
-        elif hash_version in (2, 3):
+        elif hash_version in (2, 3, 4):
             return ['name', 'debit', 'credit', 'account_id', 'partner_id']
         raise NotImplementedError(f"hash_version={hash_version} doesn't exist")
 

--- a/addons/account/models/company.py
+++ b/addons/account/models/company.py
@@ -353,6 +353,41 @@ class ResCompany(models.Model):
                 action_error = self._get_fiscalyear_lock_statement_lines_redirect_action(unreconciled_statement_lines)
                 raise RedirectWarning(error_msg, action_error, _('Show Unreconciled Bank Statement Line'))
 
+            # Check if there are still unhashed journal entries
+            # Only check journals that have at least one hashed entry.
+            journals_to_check = self.env['account.journal']
+            for journal in self.env['account.journal'].search([
+                ('restrict_mode_hash_table', '=', True),
+            ]):
+                if self.env['account.move'].search_count([
+                    ('inalterable_hash', '!=', False),
+                    ('journal_id', '=', journal.id),
+                ], limit=1):
+                    journals_to_check |= journal
+
+            chains_to_hash = self.env['account.move'].search([
+                ('restrict_mode_hash_table', '=', True),
+                ('inalterable_hash', '=', False),
+                ('journal_id', 'in', journals_to_check.ids),
+                ('date', '<=', values['fiscalyear_lock_date']),
+            ])._get_chains_to_hash(force_hash=True, raise_if_no_document=False)
+            move_ids = [move.id for chain in chains_to_hash for move in chain['moves']]
+            if move_ids:
+                msg = _("Some journal entries have not been hashed yet. You should hash them before locking the fiscal year.")
+                action = {
+                    'type': 'ir.actions.act_window',
+                    'name': _('Journal Entries to Hash'),
+                    'res_model': 'account.move',
+                    'domain': [('id', 'in', move_ids)],
+                    'views': [(False, 'tree'), (False, 'form')],
+                }
+                if len(move_ids) == 1:
+                    action.update({
+                        'res_id': move_ids[0],
+                        'views': [(False, 'form')],
+                    })
+                raise RedirectWarning(msg, action, _('Show Journal Entries to Hash'))
+
     def _get_user_fiscal_lock_date(self):
         """Get the fiscal lock date for this company depending on the user"""
         lock_date = max(self.period_lock_date or date.min, self.fiscalyear_lock_date or date.min)
@@ -595,85 +630,100 @@ class ResCompany(models.Model):
         if not self.env.user.has_group('account.group_account_user'):
             raise UserError(_('Please contact your accountant to print the Hash integrity result.'))
 
-        def build_move_info(move):
-            return(move.name, move.inalterable_hash, fields.Date.to_string(move.date))
-
         journals = self.env['account.journal'].search(self.env['account.journal']._check_company_domain(self))
-        results_by_journal = {
-            'results': [],
-            'printing_date': format_date(self.env, fields.Date.to_string(fields.Date.context_today(self)))
-        }
+        results = []
 
         for journal in journals:
-            rslt = {
-                'journal_name': journal.name,
-                'journal_code': journal.code,
-                'restricted_by_hash_table': journal.restrict_mode_hash_table and 'V' or 'X',
-                'msg_cover': '',
-                'first_hash': 'None',
-                'first_move_name': 'None',
-                'first_move_date': 'None',
-                'last_hash': 'None',
-                'last_move_name': 'None',
-                'last_move_date': 'None',
-            }
             if not journal.restrict_mode_hash_table:
-                rslt.update({'msg_cover': _('This journal is not in strict mode.')})
-                results_by_journal['results'].append(rslt)
+                results.append({
+                    'journal_name': journal.name,
+                    'restricted_by_hash_table': 'X',
+                    'status': 'not_restricted',
+                    'msg_cover': _('This journal is not restricted'),
+                })
                 continue
 
             # We need the `sudo()` to ensure that all the moves are searched, no matter the user's access rights.
-            # This is required in order to generate consistent hashs.
+            # This is required in order to generate consistent hashes.
             # It is not an issue, since the data is only used to compute a hash and not to return the actual values.
-            domain = self.env['account.move']._get_move_hash_domain(common_domain=[('journal_id', '=', journal.id)])
-            all_moves_count = self.env['account.move'].sudo().search_count(domain)
-            domain = expression.AND([domain, [('secure_sequence_number', '!=', 0)]])
-            moves = self.env['account.move'].sudo().search(domain, order="secure_sequence_number ASC")
+            field_names = [
+                *self.env['account.move']._get_integrity_hash_fields(),
+                'inalterable_hash',
+                'journal_id',
+                'move_type',
+                'sequence_prefix',
+                'sequence_number',
+                'secure_sequence_number',
+            ]
+            moves = self.env['account.move'].sudo().search_fetch(
+                domain=[
+                    ('journal_id', '=', journal.id),
+                    ('inalterable_hash', '!=', False),
+                ],
+                field_names=field_names,
+                order="secure_sequence_number ASC NULLS LAST, sequence_prefix, sequence_number ASC"
+            )
+
             if not moves:
-                rslt.update({
-                    'msg_cover': _('There isn\'t any journal entry flagged for data inalterability yet for this journal.'),
+                results.append({
+                    'journal_name': journal.name,
+                    'restricted_by_hash_table': 'V',
+                    'status': 'no_data',
+                    'msg_cover': _('There is no journal entry flagged for accounting data inalterability yet.'),
                 })
-                results_by_journal['results'].append(rslt)
                 continue
 
-            previous_hash = u''
-            start_move_info = []
-            hash_corrupted = False
+            prefix2result = defaultdict(lambda: {
+                'first_move': self.env['account.move'],
+                'last_move': self.env['account.move'],
+                'corrupted_move': self.env['account.move'],
+            })
+            last_move = self.env['account.move']
             current_hash_version = 1
             for move in moves:
-                computed_hash = move.with_context(hash_version=current_hash_version)._compute_hash(previous_hash=previous_hash)
+                prefix_result = prefix2result[move.sequence_prefix]
+                if prefix_result['corrupted_move']:
+                    continue
+                previous_move = prefix_result['last_move'] if not move.secure_sequence_number else last_move
+                previous_hash = previous_move.inalterable_hash or ""
+                computed_hash = move.with_context(hash_version=current_hash_version)._calculate_hashes(previous_hash)[move]
                 while move.inalterable_hash != computed_hash and current_hash_version < MAX_HASH_VERSION:
                     current_hash_version += 1
-                    computed_hash = move.with_context(hash_version=current_hash_version)._compute_hash(previous_hash=previous_hash)
+                    computed_hash = move.with_context(hash_version=current_hash_version)._calculate_hashes(previous_hash)[move]
                 if move.inalterable_hash != computed_hash:
-                    rslt.update({'msg_cover': _('Corrupted data on journal entry with id %s.', move.id)})
-                    results_by_journal['results'].append(rslt)
-                    hash_corrupted = True
-                    break
-                if not previous_hash:
-                    #save the date and sequence number of the first move hashed
-                    start_move_info = build_move_info(move)
-                previous_hash = move.inalterable_hash
-            end_move_info = build_move_info(move)
-
-            if hash_corrupted:
-                continue
-
-            rslt.update({
-                        'first_move_name': start_move_info[0],
-                        'first_hash': start_move_info[1],
-                        'first_move_date': format_date(self.env, start_move_info[2]),
-                        'last_move_name': end_move_info[0],
-                        'last_hash': end_move_info[1],
-                        'last_move_date': format_date(self.env, end_move_info[2]),
+                    prefix_result['corrupted_move'] = move
+                    continue
+                if not prefix_result['first_move']:
+                    prefix_result['first_move'] = move
+                prefix_result['last_move'] = move
+                last_move = move
+
+            for prefix, prefix_result in prefix2result.items():
+                if corrupted_move := prefix_result['corrupted_move']:
+                    results.append({
+                        'restricted_by_hash_table': 'V',
+                        'journal_name': f"{journal.name} ({prefix}...)",
+                        'status': 'corrupted',
+                        'msg_cover': _("Corrupted data on journal entry with id %s (%s).", corrupted_move.id, corrupted_move.name),
+                    })
+                else:
+                    results.append({
+                        'restricted_by_hash_table': 'V',
+                        'journal_name': f"{journal.name} ({prefix}...)",
+                        'status': 'verified',
+                        'msg_cover': _("Entries are correctly hashed"),
+                        'first_move_name': prefix_result['first_move'].name,
+                        'first_hash': prefix_result['first_move'].inalterable_hash,
+                        'first_move_date': format_date(self.env, prefix_result['first_move'].date),
+                        'last_move_name': prefix_result['last_move'].name,
+                        'last_hash': prefix_result['last_move'].inalterable_hash,
+                        'last_move_date': format_date(self.env, prefix_result['last_move'].date),
                     })
-            if len(moves) == all_moves_count:
-                rslt['msg_cover'] = _('All entries are hashed.')
-            else:
-                rslt['msg_cover'] = _('Entries are hashed from %s (%s)', start_move_info[0], format_date(self.env, start_move_info[2]))
-            results_by_journal['results'].append(rslt)
 
-        return results_by_journal
+        return {
+            'results': results,
+            'printing_date': format_date(self.env, fields.Date.context_today(self)),
+        }
 
     def compute_fiscalyear_dates(self, current_date):
         """
