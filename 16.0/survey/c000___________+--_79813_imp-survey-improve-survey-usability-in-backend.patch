PR: https://github.com/odoo/odoo/pull/79813

From: 1fb99d79b1567d4c150dc07f309ce54a3beea8e3
From: wde-odoo
Date: 2021-12-01 18:39:45

Structural Changes: 11
Total Changes: 69

[IMP] survey: improve survey usability in backend

PURPOSE

This commit globally improves the backend survey module usability, notably by
renaming labels, moving menu items around and cleaning unused features.

SPECIFICATIONS

These changes include (non-exhaustive list):

On questions

  - Changing labels, action helpers and modules description to be clearer;
  - Re-organizing the survey.question form view, notably to clearly distinguish
    fields related to answers and validation options;
  - Removing the "Clean test answers" server action as it can be done through
    a search + unlink;
  - Removing the allow_value_image field as we now always display the
    image field. Users simply choose to let it blank;

On surveys

  - Re-organizing the survey.survey form view fields to get a clear view of
    the various options;
  - automatically update scoring type when checking certification: if not one
    linked to scoring, update to scoring without answers;
  - set create as create_multi to speedup batch creation;
  - Changes ACL rights for user_input_line. Now, only Survey Managers can
    change answers. Survey users keep only a read access on answers, meaning
    changing what customers / people answered is now limited to managers;

Globally

  - Moving menu items and make them visible outside debug mode;
  - Answer recap on print frontend page is now visible whenever a scoring is
    applied, not only for certification, as if scoring is activated seeing
    answers is probably wanted;

Task-2600241

closes odoo/odoo#79813

Related: odoo/upgrade#3033
Signed-off-by: Thibault Delavallee (tde) <tde@openerp.com>

================================= pseudo patch: =================================

--- a/addons/survey/models/survey_question.py
+++ b/addons/survey/models/survey_question.py
@@ -49,7 +49,7 @@ class SurveyQuestion(models.Model):
     def default_get(self, fields):
         defaults = super(SurveyQuestion, self).default_get(fields)
         if (not fields or 'question_type' in fields):
-            defaults['question_type'] = False if defaults.get('is_page') == True else 'text_box'
+            defaults['question_type'] = False if defaults.get('is_page') else 'simple_choice'
         return defaults
 
     # question generic data
@@ -70,18 +70,18 @@ class SurveyQuestion(models.Model):
         related='survey_id.questions_selection', readonly=True,
         help="If randomized is selected, add the number of random questions next to the section.")
     random_questions_count = fields.Integer(
-        'Random questions count', default=1,
+        '# Questions Randomly Picked', default=1,
         help="Used on randomized sections to take X random questions from all the questions of that section.")
     # question specific
     page_id = fields.Many2one('survey.question', string='Page', compute="_compute_page_id", store=True)
     question_type = fields.Selection([
+        ('simple_choice', 'Multiple choice: only one answer'),
+        ('multiple_choice', 'Multiple choice: multiple answers allowed'),
         ('text_box', 'Multiple Lines Text Box'),
         ('char_box', 'Single Line Text Box'),
         ('numerical_box', 'Numerical Value'),
         ('date', 'Date'),
         ('datetime', 'Datetime'),
-        ('simple_choice', 'Multiple choice: only one answer'),
-        ('multiple_choice', 'Multiple choice: multiple answers allowed'),
         ('matrix', 'Matrix')], string='Question Type',
         compute='_compute_question_type', readonly=False, store=True)
     is_scored_question = fields.Boolean(
@@ -104,7 +104,6 @@ class SurveyQuestion(models.Model):
     suggested_answer_ids = fields.One2many(
         'survey.question.answer', 'question_id', string='Types of answers', copy=True,
         help='Labels used for proposed choices: simple choice, multiple choice and columns of matrix')
-    allow_value_image = fields.Boolean('Images on answers', help='Display images in addition to answer label. Valid only for simple / multiple choice questions.')
     # -- matrix
     matrix_subtype = fields.Selection([
         ('simple', 'One choice per row'),
@@ -125,7 +124,7 @@ class SurveyQuestion(models.Model):
     comments_message = fields.Char('Comment Message', translate=True, default=lambda self: _("If other, please specify:"))
     comment_count_as_answer = fields.Boolean('Comment Field is an Answer Choice')
     # question validation
-    validation_required = fields.Boolean('Validate entry')
+    validation_required = fields.Boolean('Validate entry', compute='_compute_validation_required', readonly=False, store=True)
     validation_email = fields.Boolean('Input must be an email')
     validation_length_min = fields.Integer('Minimum Text Length', default=0)
     validation_length_max = fields.Integer('Maximum Text Length', default=0)
@@ -260,6 +259,12 @@ class SurveyQuestion(models.Model):
             if question.question_type != 'char_box':
                 question.save_as_nickname = False
 
+    @api.depends('question_type')
+    def _compute_validation_required(self):
+        for question in self:
+            if not question.validation_required or question.question_type not in ['char_box', 'numerical_box', 'date', 'datetime']:
+                question.validation_required = False
+
     @api.depends('is_conditional')
     def _compute_triggering_question_id(self):
         """ Used as an 'onchange' : Reset the triggering question if user uncheck 'Conditional Display'
@@ -584,13 +589,17 @@ class SurveyQuestionAnswer(models.Model):
     _order = 'sequence, id'
     _description = 'Survey Label'
 
+    # question and question related fields
     question_id = fields.Many2one('survey.question', string='Question', ondelete='cascade')
     matrix_question_id = fields.Many2one('survey.question', string='Question (as matrix row)', ondelete='cascade')
+    question_type = fields.Selection(related='question_id.question_type')
     sequence = fields.Integer('Label Sequence order', default=10)
+    scoring_type = fields.Selection(related='question_id.scoring_type')
+    # answer related fields
     value = fields.Char('Suggested value', translate=True, required=True)
     value_image = fields.Image('Image', max_width=256, max_height=256)
-    is_correct = fields.Boolean('Is a correct answer')
-    answer_score = fields.Float('Score for this choice', help="A positive score indicates a correct choice; a negative or null score indicates a wrong answer")
+    is_correct = fields.Boolean('Correct')
+    answer_score = fields.Float('Score', help="A positive score indicates a correct choice; a negative or null score indicates a wrong answer")
 
     @api.constrains('question_id', 'matrix_question_id')
     def _check_question_not_empty(self):

--- a/addons/survey/models/survey_survey.py
+++ b/addons/survey/models/survey_survey.py
@@ -70,15 +70,15 @@ class Survey(models.Model):
         ('one_page', 'One page with all the questions'),
         ('page_per_section', 'One page per section'),
         ('page_per_question', 'One page per question')],
-        string="Layout", required=True, default='one_page')
+        string="Pagination", required=True, default='one_page')
     questions_selection = fields.Selection([
         ('all', 'All questions'),
-        ('random', 'Randomized per section')],
-        string="Selection", required=True, default='all',
+        ('random', 'Randomized per Section')],
+        string="Question Selection", required=True, default='all',
         help="If randomized is selected, you can configure the number of random questions by section. This mode is ignored in live session.")
     progression_mode = fields.Selection([
-        ('percent', 'Percentage'),
-        ('number', 'Number')], string='Progression Mode', default='percent',
+        ('percent', 'Percentage left'),
+        ('number', 'Number')], string='Display Progress as', default='percent',
         help="If Number is selected, it will display the number of questions answered on the total number of question to answer.")
     # attendees
     user_input_ids = fields.One2many('survey.user_input', 'survey_id', string='User responses', readonly=True, groups='survey.group_survey_user')
@@ -88,23 +88,23 @@ class Survey(models.Model):
         ('token', 'Invited people only')], string='Access Mode',
         default='public', required=True)
     access_token = fields.Char('Access Token', default=lambda self: self._get_default_access_token(), copy=False)
-    users_login_required = fields.Boolean('Login Required', help="If checked, users have to login before answering even with a valid token.")
+    users_login_required = fields.Boolean('Require Login', help="If checked, users have to login before answering even with a valid token.")
     users_can_go_back = fields.Boolean('Users can go back', help="If checked, users can go back to previous pages.")
     users_can_signup = fields.Boolean('Users can signup', compute='_compute_users_can_signup')
     # statistics
     answer_count = fields.Integer("Registered", compute="_compute_survey_statistic")
     answer_done_count = fields.Integer("Attempts", compute="_compute_survey_statistic")
-    answer_score_avg = fields.Float("Avg Score %", compute="_compute_survey_statistic")
+    answer_score_avg = fields.Float("Avg Score (%)", compute="_compute_survey_statistic")
     answer_duration_avg = fields.Float("Average Duration", compute="_compute_answer_duration_avg", help="Average duration of the survey (in hours)")
     success_count = fields.Integer("Success", compute="_compute_survey_statistic")
-    success_ratio = fields.Integer("Success Ratio", compute="_compute_survey_statistic")
+    success_ratio = fields.Integer("Success Ratio (%)", compute="_compute_survey_statistic")
     # scoring
     scoring_type = fields.Selection([
         ('no_scoring', 'No scoring'),
         ('scoring_with_answers', 'Scoring with answers at the end'),
         ('scoring_without_answers', 'Scoring without answers at the end')],
-        string="Scoring", required=True, default='no_scoring')
-    scoring_success_min = fields.Float('Success %', default=80.0)
+        string="Scoring", required=True, store=True, readonly=False, compute="_compute_scoring_type", precompute=True)
+    scoring_success_min = fields.Float('Required Score (%)', default=80.0)
     # attendees context: attempts and time limitation
     is_attempts_limited = fields.Boolean('Limited number of attempts', help="Check this option if you want to limit the number of attempts per user",
                                          compute="_compute_is_attempts_limited", store=True, readonly=False)
@@ -113,9 +113,9 @@ class Survey(models.Model):
     time_limit = fields.Float("Time limit (minutes)", default=10)
     # certification
     certification = fields.Boolean('Is a Certification', compute='_compute_certification',
-                                   readonly=False, store=True)
+                                   readonly=False, store=True, precompute=True)
     certification_mail_template_id = fields.Many2one(
-        'mail.template', 'Email Template',
+        'mail.template', 'Certified Email Template',
         domain="[('model', '=', 'survey.user_input')]",
         help="Automated email sent to the user when he succeeds the certification, containing his certification document.")
     certification_report_layout = fields.Selection([
@@ -324,16 +324,24 @@ class Survey(models.Model):
                not survey.certification:
                 survey.certification_give_badge = False
 
+    @api.depends('certification')
+    def _compute_scoring_type(self):
+        for survey in self:
+            if survey.certification and survey.scoring_type not in ['scoring_without_answers', 'scoring_with_answers']:
+                survey.scoring_type = 'scoring_without_answers'
+            elif not survey.scoring_type:
+                survey.scoring_type = 'no_scoring'
+
     # ------------------------------------------------------------
     # CRUD
     # ------------------------------------------------------------
 
-    @api.model
-    def create(self, vals):
-        survey = super(Survey, self).create(vals)
-        if vals.get('certification_give_badge'):
-            survey.sudo()._create_certification_badge_trigger()
-        return survey
+    @api.model_create_multi
+    def create(self, vals_list):
+        surveys = super(Survey, self).create(vals_list)
+        for survey_sudo in surveys.filtered(lambda survey: survey.certification_give_badge).sudo():
+            survey_sudo._create_certification_badge_trigger()
+        return surveys
 
     def write(self, vals):
         result = super(Survey, self).write(vals)
