PR: https://github.com/odoo/odoo/pull/

From: 755ac5f313fd16031728055530db8ed7260acef8
From: Thibault Delavallée
Date: 2019-12-05 15:21:28

Structural Changes: 8
Total Changes: 58

[REF] survey: on survey.user_input, rename some fields to ease understanding

PURPOSE

As new features are about to land in survey, notably live interactions [1]
and new survey building [2] performing a pre cleaning is necessary. In this
PR we clean survey models by: removing unnecessary fields, cleaning some code
and finally renaming models.

SPECIFICATIONS: RENAME QUIZ_SCORE ON SURVEY.USER_INPUT

On survey.user_input, quiz_score field to scoring_percentage

``quiz_score`` field name is related to the old "quiz" behavior of surveys
that is replaced by certifications and scoring mechanisms. Let us propagate
the renaming, beginning with a ``scoring_`` prefix.

SPECIFICATIONS: RENAME QUIZZ_PASSED ON SURVEY.USER_INPUT

on survey.user_input, rename quizz_passed field to scoring_success

``quizz_passed`` field name is related to the old "quiz" behavior of surveys
that is replaced by certifications and scoring mechanisms. Let us propagate
the renaming, beginning with a ``scoring_`` prefix.

SPECIFICATIONS: RENAME TOKEN ON SURVEY.USER_INPUT

on survey.user_input, rename token field to access_token

Survey user input model holds two token field. One is used to distinguish
a pool of attempts linked to a given invite: ``invite_token``. The other
one is used to control access to a specific user input. It means that
``invite_token`` indicates a set of user inputs and each of them is accessed
through its own ``token``. To be coherent with other naming in odoo this
latter field is renamed to ``access_token``.

LINKS

[0] Related to Task ID 2061901 (survey models cleaning and preparation)
[1] Task ID 1972640 (live interactions)
[2] Task ID 2119587 (new frontend for building surveys)

PR #40765

================================= pseudo patch: =================================

--- a/addons/survey/models/res_partner.py
+++ b/addons/survey/models/res_partner.py
@@ -13,7 +13,7 @@ class ResPartner(models.Model):
     @api.depends('is_company')
     def _compute_certifications_count(self):
         read_group_res = self.env['survey.user_input'].sudo().read_group(
-            [('partner_id', 'in', self.ids), ('quizz_passed', '=', True)],
+            [('partner_id', 'in', self.ids), ('scoring_success', '=', True)],
             ['partner_id'], 'partner_id'
         )
         data = dict((res['partner_id'][0], res['partner_id_count']) for res in read_group_res)

--- a/addons/survey/models/survey_survey.py
+++ b/addons/survey/models/survey_survey.py
@@ -122,7 +122,7 @@ class Survey(models.Model):
         for survey in self:
             survey.users_can_signup = signup_allowed
 
-    @api.depends('user_input_ids.state', 'user_input_ids.test_entry', 'user_input_ids.quizz_score', 'user_input_ids.quizz_passed')
+    @api.depends('user_input_ids.state', 'user_input_ids.test_entry', 'user_input_ids.scoring_percentage', 'user_input_ids.scoring_success')
     def _compute_survey_statistic(self):
         default_vals = {
             'answer_count': 0, 'answer_done_count': 0, 'success_count': 0,
@@ -132,13 +132,13 @@ class Survey(models.Model):
         UserInput = self.env['survey.user_input']
         base_domain = ['&', ('survey_id', 'in', self.ids), ('test_entry', '!=', True)]
 
-        read_group_res = UserInput.read_group(base_domain, ['survey_id', 'state'], ['survey_id', 'state', 'quizz_score', 'quizz_passed'], lazy=False)
+        read_group_res = UserInput.read_group(base_domain, ['survey_id', 'state'], ['survey_id', 'state', 'scoring_percentage', 'scoring_success'], lazy=False)
         for item in read_group_res:
             stat[item['survey_id'][0]]['answer_count'] += item['__count']
-            stat[item['survey_id'][0]]['answer_score_avg_total'] += item['quizz_score']
+            stat[item['survey_id'][0]]['answer_score_avg_total'] += item['scoring_percentage']
             if item['state'] == 'done':
                 stat[item['survey_id'][0]]['answer_done_count'] += item['__count']
-            if item['quizz_passed']:
+            if item['scoring_success']:
                 stat[item['survey_id'][0]]['success_count'] += item['__count']
 
         for survey_id, values in stat.items():
@@ -470,7 +470,7 @@ class Survey(models.Model):
     def action_start_survey(self, answer=None):
         """ Open the website page with the survey form """
         self.ensure_one()
-        url = '%s?%s' % (self.get_start_url(), werkzeug.urls.url_encode({'answer_token': answer.token or None}))
+        url = '%s?%s' % (self.get_start_url(), werkzeug.urls.url_encode({'answer_token': answer.access_token or None}))
         return {
             'type': 'ir.actions.act_url',
             'name': "Start Survey",
@@ -481,7 +481,7 @@ class Survey(models.Model):
     def action_print_survey(self, answer=None):
         """ Open the website page with the survey printable view """
         self.ensure_one()
-        url = '%s?%s' % (self.get_print_url(), werkzeug.urls.url_encode({'answer_token': answer.token or None}))
+        url = '%s?%s' % (self.get_print_url(), werkzeug.urls.url_encode({'answer_token': answer.access_token or None}))
         return {
             'type': 'ir.actions.act_url',
             'name': "Print Survey",
@@ -524,7 +524,7 @@ class Survey(models.Model):
         action = action_rec.read()[0]
         ctx = dict(self.env.context)
         ctx.update({'search_default_survey_id': self.ids[0],
-                    'search_default_quizz_passed': 1,
+                    'search_default_scoring_success': 1,
                     'search_default_not_test': 1})
         action['context'] = ctx
         return action
@@ -724,7 +724,7 @@ class Survey(models.Model):
         goal = self.env['gamification.goal.definition'].create({
             'name': self.title,
             'description': "%s certification passed" % self.title,
-            'domain': "['&', ('survey_id', '=', %s), ('quizz_passed', '=', True)]" % self.id,
+            'domain': "['&', ('survey_id', '=', %s), ('scoring_success', '=', True)]" % self.id,
             'computation_mode': 'count',
             'display_mode': 'boolean',
             'model_id': self.env.ref('survey.model_survey_user_input').id,

--- a/addons/survey/models/survey_user.py
+++ b/addons/survey/models/survey_user.py
@@ -32,25 +32,25 @@ class SurveyUserInput(models.Model):
     # attempts management
     is_attempts_limited = fields.Boolean("Limited number of attempts", related='survey_id.is_attempts_limited')
     attempts_limit = fields.Integer("Number of attempts", related='survey_id.attempts_limit')
-    attempt_number = fields.Integer("Attempt n°", compute='_compute_attempt_number')
+    attempts_number = fields.Integer("Attempt n°", compute='_compute_attempts_number')
     is_time_limit_reached = fields.Boolean("Is time limit reached?", compute='_compute_is_time_limit_reached')
     # identification / access
-    token = fields.Char('Identification token', default=lambda self: str(uuid.uuid4()), readonly=True, required=True, copy=False)
+    access_token = fields.Char('Identification token', default=lambda self: str(uuid.uuid4()), readonly=True, required=True, copy=False)
     invite_token = fields.Char('Invite token', readonly=True, copy=False)  # no unique constraint, as it identifies a pool of attempts
     partner_id = fields.Many2one('res.partner', string='Partner', readonly=True)
     email = fields.Char('E-mail', readonly=True)
     # questions / answers
     user_input_line_ids = fields.One2many('survey.user_input.line', 'user_input_id', string='Answers', copy=True)
     question_ids = fields.Many2many('survey.question', string='Predefined Questions', readonly=True)
-    quizz_score = fields.Float("Score (%)", compute="_compute_quizz_score", store=True, compute_sudo=True)  # stored for perf reasons
-    quizz_passed = fields.Boolean('Quizz Passed', compute='_compute_quizz_passed', store=True, compute_sudo=True)  # stored for perf reasons
+    scoring_percentage = fields.Float("Score (%)", compute="_compute_scoring_percentage", store=True, compute_sudo=True)  # stored for perf reasons
+    scoring_success = fields.Boolean('Quizz Passed', compute='_compute_scoring_success', store=True, compute_sudo=True)  # stored for perf reasons
 
     _sql_constraints = [
-        ('unique_token', 'UNIQUE (token)', 'A token must be unique!'),
+        ('unique_token', 'UNIQUE (access_token)', 'An access token must be unique!'),
     ]
 
     @api.depends('user_input_line_ids.answer_score', 'user_input_line_ids.question_id')
-    def _compute_quizz_score(self):
+    def _compute_scoring_percentage(self):
         for user_input in self:
             total_possible_score = sum([
                 answer_score if answer_score > 0 else 0
@@ -58,15 +58,15 @@ class SurveyUserInput(models.Model):
             ])
 
             if total_possible_score == 0:
-                user_input.quizz_score = 0
+                user_input.scoring_percentage = 0
             else:
                 score = (sum(user_input.user_input_line_ids.mapped('answer_score')) / total_possible_score) * 100
-                user_input.quizz_score = round(score, 2) if score > 0 else 0
+                user_input.scoring_percentage = round(score, 2) if score > 0 else 0
 
-    @api.depends('quizz_score', 'survey_id.scoring_success_min')
-    def _compute_quizz_passed(self):
+    @api.depends('scoring_percentage', 'survey_id.scoring_success_min')
+    def _compute_scoring_success(self):
         for user_input in self:
-            user_input.quizz_passed = user_input.quizz_score >= user_input.survey_id.scoring_success_min
+            user_input.scoring_success = user_input.scoring_percentage >= user_input.survey_id.scoring_success_min
 
     @api.depends('start_datetime', 'survey_id.is_time_limited', 'survey_id.time_limit')
     def _compute_is_time_limit_reached(self):
@@ -76,16 +76,16 @@ class SurveyUserInput(models.Model):
                 > user_input.start_datetime + relativedelta(minutes=user_input.survey_id.time_limit)
 
     @api.depends('state', 'test_entry', 'survey_id.is_attempts_limited', 'partner_id', 'email', 'invite_token')
-    def _compute_attempt_number(self):
+    def _compute_attempts_number(self):
         attempts_to_compute = self.filtered(
             lambda user_input: user_input.state == 'done' and not user_input.test_entry and user_input.survey_id.is_attempts_limited
         )
 
         for user_input in (self - attempts_to_compute):
-            user_input.attempt_number = 1
+            user_input.attempts_number = 1
 
         if attempts_to_compute:
-            self.env.cr.execute("""SELECT user_input.id, (COUNT(previous_user_input.id) + 1) AS attempt_number
+            self.env.cr.execute("""SELECT user_input.id, (COUNT(previous_user_input.id) + 1) AS attempts_number
                 FROM survey_user_input user_input
                 LEFT OUTER JOIN survey_user_input previous_user_input
                 ON user_input.survey_id = previous_user_input.survey_id
@@ -101,13 +101,13 @@ class SurveyUserInput(models.Model):
             attempts_count_results = self.env.cr.dictfetchall()
 
             for user_input in attempts_to_compute:
-                attempt_number = 1
+                attempts_number = 1
                 for attempts_count_result in attempts_count_results:
                     if attempts_count_result['id'] == user_input.id:
-                        attempt_number = attempts_count_result['attempt_number']
+                        attempts_number = attempts_count_result['attempts_number']
                         break
 
-                user_input.attempt_number = attempt_number
+                user_input.attempts_number = attempts_number
 
     def action_resend(self):
         partners = self.env['res.partner']
@@ -131,7 +131,7 @@ class SurveyUserInput(models.Model):
             'type': 'ir.actions.act_url',
             'name': "View Answers",
             'target': 'self',
-            'url': '/survey/print/%s?answer_token=%s' % (self.survey_id.access_token, self.token)
+            'url': '/survey/print/%s?answer_token=%s' % (self.survey_id.access_token, self.access_token)
         }
 
     @api.model
@@ -150,7 +150,7 @@ class SurveyUserInput(models.Model):
         Challenge = self.env['gamification.challenge'].sudo()
         badge_ids = []
         for user_input in self:
-            if user_input.survey_id.certification and user_input.quizz_passed:
+            if user_input.survey_id.certification and user_input.scoring_success:
                 if user_input.survey_id.certification_mail_template_id and not user_input.test_entry:
                     user_input.survey_id.certification_mail_template_id.send_mail(user_input.id, notif_layout="mail.mail_notification_light")
                 if user_input.survey_id.certification_give_badge:
@@ -163,7 +163,7 @@ class SurveyUserInput(models.Model):
 
     def get_start_url(self):
         self.ensure_one()
-        return '%s?answer_token=%s' % (self.survey_id.get_start_url(), self.token)
+        return '%s?answer_token=%s' % (self.survey_id.get_start_url(), self.access_token)
 
     def get_print_url(self):
         self.ensure_one()
