PR: https://github.com/odoo/odoo/pull/

From: afb1abbb60e5cdc4f76ab7b5a1b54b5e8a9283d5
From: AurÃ©lien Warnon
Date: 2019-02-06 10:34:40

Breaking data model changes score: 3, change matches:
-    quizz_mark = fields.Float('Score for this choice', help="A positive score indicates a correct choice; a negative or null score indicates a wrong answer")
-    quizz_mode = fields.Boolean("Quizz Mode")
-    quizz_mark = fields.Float('Score given for this choice')

Total Changes: 164

[IMP] survey: scoring & certification

Task #1902306

Purpose
=======

This commit adds certification capabilities to the survey module.

A certification is a survey with the "certification" flag set to true than can be linked
to a certification email template that contains a certification document (PDF).
This template can be edited by the user in the technical settings to customize the certification email/document.

To be able to implement this certification concept, we also have to add scoring mechanisms to surveys.
survey.questions of type 'simple_choice' and 'multiple_choice' now have scores for the suggested answers.
These question scores allow to compute a global score that is used to determine whether the user has successfully
passed the certification or not.

As additional features, we also have:
- A time limit with an interface timer that limits the test to X minutes
  When reached, the survey is automatically submitted and unregistered (= unsubmitted) answers are not taken into account.
- A limited number of attempts for the survey/certification
  If reached, the user can't take the survey/certification anymore

The 'survey result' layout was adapted accordingly to show the success rate of participants and the correct answers
to the survey questions.

Specs
=======
- Create a new survey :
  - Add a description field for the survey

- Options on a survey :
  - Passing score : (sum of all good answers) in %
  - If No scoring => No passing score, no certificate
  - If Scoring with answers => Passing score and can see the answers (can create certificate)
  - If Scoring without answers => Passing score but can't review the answers at the end (can create certificate)
  - for the questions, if "no scoring" selected, can 't see the option "good answer" and "score" on the questions
  - If the 2 others options, can see the options "good answer" and "score" on the questions
  - all the types of questions are available for each option.

- Questions
  - Add the option correct answer on the multiples questions (one or more good answers)

- Certifications
  - If scoring, force "mandatory" for "mutliple choices (1or multiple answers)"
  - On the dashboard => visual information that this specific survey is a certification
  - Time Limit : The student is informed on the home screen of the survey of the time limit.
    The clock start when he clicks on "start survey"
  - Template of the certificate : send email with attachment PDF

- Front-end :
  - Add some margin
  - Replace "Back to survey" with the blue-bar from the portal
  - Add a timer (start when the survey starts)
  - Add a progress bar (number of section and number of question inside the section)

- Analyse of the results :
  - First a global graph with the number of people who''ve participated and passed the test

- Stages of a survey :
  - Remove the stage "Permanent"
  - 3 stages :
    - draft : not on-line but can be tested (with phantom token)
    - In progress : on-line
    - closed : not on-line
  - Who can test a survey : the manager and the user. Add this condition to the phantom token.

================================= pseudo patch: =================================

--- a/addons/survey/models/survey_question.py
+++ b/addons/survey/models/survey_question.py
@@ -54,6 +54,7 @@ class SurveyQuestion(models.Model):
     survey_id = fields.Many2one('survey.survey', string='Survey', ondelete='cascade')
     page_id = fields.Many2one('survey.question', string='Page', compute="_compute_page_id", store=True)
     question_ids = fields.One2many('survey.question', string='Questions', compute="_compute_question_ids")
+    scoring_type = fields.Selection(related='survey_id.scoring_type', string='Scoring Type', readonly=True)
     sequence = fields.Integer('Sequence', default=10)
     # Question
     is_page = fields.Boolean('Is a page?')
@@ -350,6 +351,11 @@ class SurveyQuestion(models.Model):
         self.ensure_one()
         return list(self.survey_id.question_and_page_ids).index(self)
 
+    def get_correct_answer_ids(self):
+        self.ensure_one()
+
+        return self.labels_ids.filtered(lambda label: label.is_correct)
+
 class SurveyLabel(models.Model):
     """ A suggested answer for a question """
     _name = 'survey.label'
@@ -361,7 +367,9 @@ class SurveyLabel(models.Model):
     question_id_2 = fields.Many2one('survey.question', string='Question 2', ondelete='cascade')
     sequence = fields.Integer('Label Sequence order', default=10)
     value = fields.Char('Suggested value', translate=True, required=True)
-    quizz_mark = fields.Float('Score for this choice', help="A positive score indicates a correct choice; a negative or null score indicates a wrong answer")
+    is_correct = fields.Boolean('Is a correct answer')
+    answer_score = fields.Float('Score for this choice',
+    help="A positive score indicates a correct choice; a negative or null score indicates a wrong answer")
 
     @api.one
     @api.constrains('question_id', 'question_id_2')

--- a/addons/survey/models/survey_survey.py
+++ b/addons/survey/models/survey_survey.py
@@ -31,7 +31,6 @@ class Survey(models.Model):
     description = fields.Html("Description", translate=True, help="A long description of the purpose of the survey")
     color = fields.Integer('Color Index', default=0)
     thank_you_message = fields.Html("Thanks Message", translate=True, help="This message will be displayed when survey is completed")
-    quizz_mode = fields.Boolean("Quizz Mode")
     active = fields.Boolean("Active", default=True)
     question_and_page_ids = fields.One2many('survey.question', 'survey_id', string='Pages and Questions', copy=True)
     page_ids = fields.One2many('survey.question', string='Pages', compute="_compute_page_and_question_ids")
@@ -60,8 +59,29 @@ class Survey(models.Model):
     answer_count = fields.Integer("Started", compute="_compute_survey_statistic")
     answer_done_count = fields.Integer("Completed", compute="_compute_survey_statistic")
 
+    # scoring and certification fields
+    scoring_type = fields.Selection([
+        ('no_scoring', 'No scoring'),
+        ('scoring_with_answers', 'Scoring with answers at the end'),
+        ('scoring_without_answers', 'Scoring without answers at the end')],
+        string="Scoring", required=True, default='no_scoring')
+    passing_score = fields.Float('Passing score (%)', required=True, default=80.0)
+    is_attempts_limited = fields.Boolean('Limited number of attempts',
+        help="Check this option if you want to limit the number of attempts per user")
+    attempts_limit = fields.Integer('Number of attempts', default=1)
+    is_time_limited = fields.Boolean('The survey is limited in time')
+    time_limit = fields.Float("Time limit (minutes)")
+    certificate = fields.Boolean('Certificate')
+    certification_mail_template_id = fields.Many2one(
+        'mail.template', 'Certification Email Template',
+        domain="[('model', '=', 'survey.user_input')]",
+        help="Automated email sent to the user when he succeeds the certification, containing his certification document.")
+
     _sql_constraints = [
-        ('access_token_unique', 'unique(access_token)', 'Access token should be unique')
+        ('access_token_unique', 'unique(access_token)', 'Access token should be unique'),
+        ('certificate_check', "CHECK( scoring_type!='no_scoring' OR certificate=False )", 'You can only create certifications for surveys that have a scoring mechanism.'),
+        ('time_limit_check', "CHECK( (is_time_limited=False) OR (time_limit is not null AND time_limit > 0) )", 'The time limit needs to be a positive number if the survey is time limited.'),
+        ('attempts_limit_check', "CHECK( (is_attempts_limited=False) OR (attempts_limit is not null AND attempts_limit > 0) )", 'The attempts limit needs to be a positive number if the survey has a limited number of attempts.')
     ]
 
     @api.multi
@@ -93,12 +113,45 @@ class Survey(models.Model):
         for survey in self:
             survey.public_url = urls.url_join(base_url, "survey/start/%s" % (survey.access_token))
 
+    @api.multi
+    @api.depends('question_and_page_ids.labels_ids.answer_score')
+    def _compute_total_possible_score(self):
+        for survey in self:
+            survey.total_possible_score = sum([
+                answer_score if answer_score > 0 else 0
+                for answer_score in survey.question_and_page_ids.mapped('labels_ids.answer_score')])
+
     @api.depends('question_and_page_ids')
     def _compute_page_and_question_ids(self):
         for survey in self:
             survey.page_ids = survey.question_and_page_ids.filtered(lambda question: question.is_page)
             survey.question_ids = survey.question_and_page_ids - survey.page_ids
 
+    @api.onchange('passing_score')
+    def _onchange_passing_score(self):
+        if self.passing_score < 0 or self.passing_score > 100:
+            self.passing_score = 80.0
+
+    @api.onchange('scoring_type')
+    def _onchange_scoring_type(self):
+        if self.scoring_type != 'no_scoring':
+            self.certificate = False
+
+    @api.onchange('users_login_required', 'access_mode')
+    def _onchange_access_mode(self):
+        if self.access_mode == 'public' and not self.users_login_required:
+            self.is_attempts_limited = False
+
+    @api.onchange('attempts_limit')
+    def _onchange_attempts_limit(self):
+        if self.attempts_limit <= 0:
+            self.attempts_limit = 1
+
+    @api.onchange('is_time_limited', 'time_limit')
+    def _onchange_time_limit(self):
+        if self.is_time_limited and (not self.time_limit or self.time_limit <= 0):
+            self.time_limit = 10
+
     @api.model
     def _read_group_stage_ids(self, stages, domain, order):
         """ Read group customization in order to display all the stages in the
@@ -126,7 +179,7 @@ class Survey(models.Model):
         self.check_access_rights('read')
         self.check_access_rule('read')
 
-        tokens = self.env['survey.user_input']
+        answers = self.env['survey.user_input']
         for survey in self:
             if partner and not user and partner.user_ids:
                 user = partner.user_ids[0]
@@ -146,9 +199,9 @@ class Survey(models.Model):
                 answer_vals['email'] = email
 
             answer_vals.update(additional_vals)
-            tokens += tokens.create(answer_vals)
+            answers += answers.create(answer_vals)
 
-        return tokens
+        return answers
 
     @api.multi
     def _check_answer_creation(self, user, partner, email, test_entry=False):
@@ -171,6 +224,8 @@ class Survey(models.Model):
                     raise UserError(_('Creating token for external people is not allowed for surveys requesting authentication.'))
             if self.access_mode == 'internal' and (not user or not user.has_group('base.group_user')):
                 raise UserError(_('Creating token for anybody else than employees is not allowed for internal surveys.'))
+            if not self._has_attempts_left(partner or (user and user.partner_id), email):
+                raise UserError(_('No attempts left.'))
 
     @api.model
     def next_page(self, user_input, page_id, go_back=False):
@@ -195,23 +250,23 @@ class Survey(models.Model):
 
         # First page
         if page_id == 0:
-            return (pages[0][1], 0, len(pages) == 1)
+            return (pages[0][1], len(pages) == 1)
 
         current_page_index = pages.index(next(p for p in pages if p[1].id == page_id))
 
         # All the pages have been displayed
         if current_page_index == len(pages) - 1 and not go_back:
-            return (None, -1, False)
+            return (None, False)
         # Let's get back, baby!
         elif go_back and survey.users_can_go_back:
-            return (pages[current_page_index - 1][1], current_page_index - 1, False)
+            return (pages[current_page_index - 1][1], False)
         else:
             # This will show the last page
             if current_page_index == len(pages) - 2:
-                return (pages[current_page_index + 1][1], current_page_index + 1, True)
+                return (pages[current_page_index + 1][1], True)
             # This will show a regular page
             else:
-                return (pages[current_page_index + 1][1], current_page_index + 1, False)
+                return (pages[current_page_index + 1][1], False)
 
     @api.multi
     def filter_input_ids(self, filters, finished=False):
@@ -278,7 +333,7 @@ class Survey(models.Model):
         # Calculate and return statistics for choice
         if question.question_type in ['simple_choice', 'multiple_choice']:
             comments = []
-            answers = OrderedDict((label.id, {'text': label.value, 'count': 0, 'answer_id': label.id}) for label in question.labels_ids)
+            answers = OrderedDict((label.id, {'text': label.value, 'count': 0, 'answer_id': label.id, 'answer_score': label.answer_score}) for label in question.labels_ids)
             for input_line in input_lines:
                 if input_line.answer_type == 'suggestion' and answers.get(input_line.value_suggested.id) and (not(current_filters) or input_line.user_input_id.id in current_filters):
                     answers[input_line.value_suggested.id]['count'] += 1
@@ -435,3 +490,30 @@ class Survey(models.Model):
                     'search_default_invite': 1})
         action['context'] = ctx
         return action
+
+    @api.multi
+    def _has_attempts_left(self, partner, email):
+        self.ensure_one()
+
+        if (self.access_mode != 'public' or self.users_login_required) and self.is_attempts_limited:
+            return self._get_number_of_attempts_lefts(partner, email) > 0
+
+        return True
+
+    @api.multi
+    def _get_number_of_attempts_lefts(self, partner, email):
+        """ Returns the number of attempts left. """
+        self.ensure_one()
+
+        domain = [
+            ('survey_id', '=', self.id),
+            ('test_entry', '=', False),
+            ('state', '=', 'done')
+        ]
+
+        if partner:
+            domain = expression.AND([domain, [('partner_id', '=', partner.id)]])
+        else:
+            domain = expression.AND([domain, [('email', '=', email)]])
+
+        return self.attempts_limit - self.env['survey.user_input'].search_count(domain)

--- a/addons/survey/models/survey_user.py
+++ b/addons/survey/models/survey_user.py
@@ -9,6 +9,8 @@ import uuid
 from odoo import api, fields, models, _
 from odoo.exceptions import ValidationError
 
+from dateutil.relativedelta import relativedelta
+
 email_validator = re.compile(r"[^@]+@[^@]+\.[^@]+")
 _logger = logging.getLogger(__name__)
 
@@ -30,6 +32,8 @@ class SurveyUserInput(models.Model):
 
     # description
     survey_id = fields.Many2one('survey.survey', string='Survey', required=True, readonly=True, ondelete='cascade')
+    start_datetime = fields.Datetime('Start date and time', readonly=True)
+    is_time_limit_reached = fields.Boolean("Is time limit reached?", compute='_compute_is_time_limit_reached')
     input_type = fields.Selection([
         ('manually', 'Manual'), ('link', 'Invitation')],
         string='Answer Type', default='manually', required=True, readonly=True,
@@ -49,12 +53,27 @@ class SurveyUserInput(models.Model):
     # answers
     user_input_line_ids = fields.One2many('survey.user_input_line', 'user_input_id', string='Answers', copy=True)
     deadline = fields.Datetime('Deadline', help="Datetime until customer can open the survey and submit answers")
-    quizz_score = fields.Float("Score for the quiz", compute="_compute_quizz_score", default=0.0)
 
-    @api.depends('user_input_line_ids.quizz_mark')
+    quizz_score = fields.Float("Score for the quiz (%)", compute="_compute_quizz_score", default=0.0)
+    # Stored for performance reasons while displaying results page
+    quizz_passed = fields.Boolean('Quizz Passed', compute='_compute_quizz_passed', store=True)
+
+    @api.multi
+    @api.depends('user_input_line_ids.answer_score', 'survey_id.total_possible_score')
     def _compute_quizz_score(self):
         for user_input in self:
-            user_input.quizz_score = sum(user_input.user_input_line_ids.mapped('quizz_mark'))
+            total_possible_score = user_input.survey_id.total_possible_score
+            if total_possible_score == 0:
+                user_input.quizz_score = 0
+            else:
+                score = (sum(user_input.user_input_line_ids.mapped('answer_score')) / total_possible_score) * 100
+                user_input.quizz_score = round(score, 2) if score > 0 else 0
+
+    @api.multi
+    @api.depends('quizz_score', 'survey_id.passing_score')
+    def _compute_quizz_passed(self):
+        for user_input in self:
+            user_input.quizz_passed = user_input.quizz_score >= user_input.survey_id.passing_score
 
     _sql_constraints = [
         ('unique_token', 'UNIQUE (token)', 'A token must be unique!'),
@@ -97,6 +116,23 @@ class SurveyUserInput(models.Model):
             'url': '/survey/print/%s?answer_token=%s' % (self.survey_id.access_token, self.token)
         }
 
+    @api.depends('start_datetime', 'survey_id.is_time_limited', 'survey_id.time_limit')
+    def _compute_is_time_limit_reached(self):
+        """ Checks that the user_input is not exceeding the survey's time limit. """
+        for user_input in self:
+            user_input.is_time_limit_reached = user_input.survey_id.is_time_limited and fields.Datetime.now() \
+                > user_input.start_datetime + relativedelta(minutes=user_input.survey_id.time_limit)
+
+    @api.multi
+    def _send_certification(self):
+        """ Will send the certification email with attached document if
+        - The survey is a certification
+        - It has a certification_mail_template_id set
+        - The user succeeded the test """
+        for user_input in self:
+            if user_input.survey_id.certificate and user_input.quizz_passed and user_input.survey_id.certification_mail_template_id:
+                user_input.survey_id.certification_mail_template_id.send_mail(user_input.id, notif_layout="mail.mail_notification_light")
+
 
 class SurveyUserInputLine(models.Model):
     _name = 'survey.user_input_line'
@@ -122,7 +158,7 @@ class SurveyUserInputLine(models.Model):
     value_free_text = fields.Text('Free Text answer')
     value_suggested = fields.Many2one('survey.label', string="Suggested answer")
     value_suggested_row = fields.Many2one('survey.label', string="Row answer")
-    quizz_mark = fields.Float('Score given for this choice')
+    answer_score = fields.Float('Score given for this choice')
 
     @api.constrains('skipped', 'answer_type')
     def _answered_or_skipped(self):
@@ -145,7 +181,7 @@ class SurveyUserInputLine(models.Model):
 
     def _get_mark(self, value_suggested):
         label = self.env['survey.label'].browse(int(value_suggested))
-        mark = label.quizz_mark if label.exists() else 0.0
+        mark = label.answer_score if label.exists() else 0.0
         return mark
 
     @api.model_create_multi
@@ -153,14 +189,14 @@ class SurveyUserInputLine(models.Model):
         for vals in vals_list:
             value_suggested = vals.get('value_suggested')
             if value_suggested:
-                vals.update({'quizz_mark': self._get_mark(value_suggested)})
+                vals.update({'answer_score': self._get_mark(value_suggested)})
         return super(SurveyUserInputLine, self).create(vals_list)
 
     @api.multi
     def write(self, vals):
         value_suggested = vals.get('value_suggested')
         if value_suggested:
-            vals.update({'quizz_mark': self._get_mark(value_suggested)})
+            vals.update({'answer_score': self._get_mark(value_suggested)})
         return super(SurveyUserInputLine, self).write(vals)
 
     @api.model
